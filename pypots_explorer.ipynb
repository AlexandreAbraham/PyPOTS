{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running example from the readme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:04:01 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Data_Beans)...\n",
      "2024-11-07 16:04:01 [INFO]: Starting preprocessing physionet_2012...\n",
      "2024-11-07 16:04:01 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2024-11-07 16:04:01 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2024-11-07 16:04:01 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2024-11-07 16:04:01 [INFO]: Loaded successfully!\n",
      "2024-11-07 16:04:11 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2024-11-07 16:04:12 [INFO]: 68864 values masked out in the val set as ground truth, take 9.98% of the original observed values\n",
      "2024-11-07 16:04:12 [INFO]: 86076 values masked out in the test set as ground truth, take 9.92% of the original observed values\n",
      "2024-11-07 16:04:12 [INFO]: Total sample number: 11988\n",
      "2024-11-07 16:04:12 [INFO]: Training set size: 7671 (63.99%)\n",
      "2024-11-07 16:04:12 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2024-11-07 16:04:12 [INFO]: Test set size: 2399 (20.01%)\n",
      "2024-11-07 16:04:12 [INFO]: Number of steps: 48\n",
      "2024-11-07 16:04:12 [INFO]: Number of features: 37\n",
      "2024-11-07 16:04:12 [INFO]: Train set missing rate: 79.73%\n",
      "2024-11-07 16:04:12 [INFO]: Validating set missing rate: 81.76%\n",
      "2024-11-07 16:04:12 [INFO]: Test set missing rate: 81.66%\n",
      "2024-11-07 16:04:12 [WARNING]: ⚠️ load_specific_dataset() will be deprecated in the near future. Data preprocessing functions are moved to BenchPOTS, which now supports processing 170+ public time-series datasets.\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing. Tedious, but PyPOTS can help.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pygrinder import mcar\n",
    "from pypots.data import load_specific_dataset\n",
    "data = load_specific_dataset('physionet_2012')  # PyPOTS will automatically download and extract it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7671, 48, 37)\n"
     ]
    }
   ],
   "source": [
    "X = data['train_X']\n",
    "scaler = data['scaler']\n",
    "X_shape = X.shape\n",
    "X = scaler.transform(X.reshape(-1,X_shape[2])).reshape(X_shape)\n",
    "\n",
    "## normalize\n",
    "normalize = True\n",
    "if normalize and False:\n",
    "    latent_dim = X.shape[2]\n",
    "    mean, std = np.zeros((1,1,latent_dim)), np.ones((1,1,latent_dim))\n",
    "    for l in range(latent_dim):\n",
    "        X_l = X[:,:,l][X[:,:,l]==X[:,:,l]] \n",
    "        if len(X_l) > 0:   \n",
    "            mean[:,:,l] = X_l.mean()    \n",
    "            std[:,:,l] = X_l.std()   \n",
    "    X = (X - mean) / std\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "rng = np.random.RandomState(0)\n",
    "qt = QuantileTransformer(n_quantiles=10, random_state=0, output_distribution = 'normal')\n",
    "\n",
    "if normalize:\n",
    "    n_batch, n_obs, latent_dim = X.shape\n",
    "    for l in range(latent_dim):\n",
    "        X_l = X[:,:,l][X[:,:,l]==X[:,:,l]] \n",
    "        qt.fit(X_l.reshape(-1,1))\n",
    "        \n",
    "        X[:,:,l] = qt.transform(X[:,:,l].reshape(-1,1)).reshape(n_batch, n_obs)\n",
    "\n",
    "\n",
    "X_ori = X  # keep X_ori for validation\n",
    "X = mcar(X, 0.1)  # randomly hold out 10% observed values as ground truth\n",
    "dataset = {\"X\": X}  # X for model input\n",
    "print(X.shape)  # (11988, 48, 37), 11988 samples and each sample has 48 time steps, 37 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:04:13 [INFO]: No given device, using default device: cpu\n",
      "2024-11-07 16:04:13 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-11-07 16:04:13 [INFO]: GP_VAE initialized with the given hyperparameters, the number of trainable parameters: 16,485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dimensions is: \n",
      "GpvaeEncoder(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (mu_layer): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (logvar_layer): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "GpvaeDecoder(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=37, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:04:20 [ERROR]: ❌ Exception: probability tensor contains either `inf`, `nan` or element < 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training got interrupted. Model was not trained. Please investigate the error printed above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:255\u001b[0m, in \u001b[0;36mGP_VAE._train_model\u001b[0;34m(self, training_loader, val_loader)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 255\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(inputs)\n\u001b[1;32m    256\u001b[0m \u001b[39m# use sum() before backward() in case of multi-gpu training\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/core.py:97\u001b[0m, in \u001b[0;36m_GP_VAE.forward\u001b[0;34m(self, inputs, training, n_sampling_times)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m---> 97\u001b[0m     elbo_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(X, missing_mask)\n\u001b[1;32m     98\u001b[0m     results[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m elbo_loss\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/nn/modules/gp_ae/backbone.py:245\u001b[0m, in \u001b[0;36mBackboneGP_VAE.forward\u001b[0;34m(self, X, missing_mask)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m# sample missing vals of X_ori from p(X^m; X^o)\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m X_sampled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_missing_vals(X_ori, missing_mask_ori)\n\u001b[1;32m    246\u001b[0m nll_sampling \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_nll(px_z, X_sampled, \u001b[39m~\u001b[39mmissing_mask_ori) \u001b[39m#? I think the mask is right ?\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/nn/modules/gp_ae/backbone.py:344\u001b[0m, in \u001b[0;36mBackboneGP_VAE.sample_missing_vals\u001b[0;34m(self, X, missing_mask)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Sample indices for each missing sample\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m sampled_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probabilities, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# Shape: [num_missing]\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# Get the observed values corresponding to the sampled indices\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 19\u001b[0m\n\u001b[1;32m      7\u001b[0m gpvae \u001b[39m=\u001b[39m GP_VAE(n_steps \u001b[39m=\u001b[39m \u001b[39m48\u001b[39m, \n\u001b[1;32m      8\u001b[0m             n_features \u001b[39m=\u001b[39m \u001b[39m37\u001b[39m, \n\u001b[1;32m      9\u001b[0m             latent_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m             optimizer \u001b[39m=\u001b[39m Adam(weight_decay \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m) \u001b[39m#, lr_scheduler = LRScheduler\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             )\n\u001b[1;32m     18\u001b[0m \u001b[39m# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m gpvae\u001b[39m.\u001b[39;49mfit(dataset)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:382\u001b[0m, in \u001b[0;36mGP_VAE.fit\u001b[0;34m(self, train_set, val_set, file_type)\u001b[0m\n\u001b[1;32m    374\u001b[0m     val_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    375\u001b[0m         val_set,\n\u001b[1;32m    376\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    377\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    378\u001b[0m         num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m \u001b[39m# Step 2: train the AE model and freeze it\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_model(training_loader, val_loader)\n\u001b[1;32m    383\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict)\n\u001b[1;32m    384\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()  \u001b[39m# set the model as eval status to freeze it.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:340\u001b[0m, in \u001b[0;36mGP_VAE._train_model\u001b[0;34m(self, training_loader, val_loader)\u001b[0m\n\u001b[1;32m    338\u001b[0m logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m❌ Exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# if no best model, raise error\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Model was not trained. Please investigate the error printed above.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[39mRuntimeWarning\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining got interrupted. Please investigate the error printed above.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel got trained and will load the best checkpoint so far for testing.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt want it, please try fit() again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training got interrupted. Model was not trained. Please investigate the error printed above."
     ]
    }
   ],
   "source": [
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "from pypots.imputation import GP_VAE\n",
    "from pypots.utils.metrics import calc_mae\n",
    "from pypots.optim.adam import Adam\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "\n",
    "gpvae = GP_VAE(n_steps = 48, \n",
    "            n_features = 37, \n",
    "            latent_size = 16, \n",
    "            epochs = 5, \n",
    "            batch_size = 8,\n",
    "            beta = .001, \n",
    "            K = 10,  \n",
    "            encoder_sizes = (64, 64), \n",
    "            decoder_sizes = (64, 64),\n",
    "            optimizer = Adam(weight_decay = 1e-4) #, lr_scheduler = LRScheduler\n",
    "            )\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "gpvae.fit(dataset)  # train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting kernel\n",
      "torch.Size([8, 48])\n",
      "torch.Size([8, 8, 8]) torch.Size([8, 48])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:347: GPInputWarning: You have passed data through a FixedNoiseGaussianLikelihood that did not match the size of the fixed noise, *and* you did not specify noise. This is treated as a no-op.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/7s/75p3d4f145qfrrzfq5vskssc0000gn/T/ipykernel_11367/1211517127.py\", line 3, in <module>\n",
      "    gpvae.fit_kernel(dataset)  # train the model on the dataset\n",
      "  File \"/Users/louis/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py\", line 514, in fit_kernel\n",
      "    self._fit_kernel(training_loader)\n",
      "  File \"/Users/louis/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py\", line 484, in _fit_kernel\n",
      "    self.gp.fit_kernel(training_loader)\n",
      "  File \"/Users/louis/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/gp_model.py\", line 171, in fit_kernel\n",
      "    loss = -self.mll[j](out, z_mu[:,:,j])\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/module.py\", line 31, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 82, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py\", line 177, in log_prob\n",
      "    diff = value - mean\n",
      "RuntimeError: The size of tensor a (48) must match the size of tensor b (8) at non-singleton dimension 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/louis/miniforge3/envs/pypots-env/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "with gpytorch.settings.cholesky_jitter(1e-4):\n",
    "    gpvae.fit_kernel(dataset)  # train the model on the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pypots.imputation.gp_ae.model.ProbabilisticGP at 0x2a16fb100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpvae.gp._fit_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GP_VAE' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m imputation \u001b[39m=\u001b[39m gpvae\u001b[39m.\u001b[39;49mimpute(dataset)  \u001b[39m# impute the originally-missing values and artificially-missing values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m indicating_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(X) \u001b[39m^\u001b[39m np\u001b[39m.\u001b[39misnan(X_ori)  \u001b[39m# indicating mask for imputation error calculation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mae \u001b[39m=\u001b[39m calc_mae(imputation, np\u001b[39m.\u001b[39mnan_to_num(X_ori), indicating_mask)  \u001b[39m# calculate mean absolute error on the ground truth (artificially-missing values)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:581\u001b[0m, in \u001b[0;36mGP_VAE.impute\u001b[0;34m(self, test_set, file_type)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimpute\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    561\u001b[0m     test_set: Union[\u001b[39mdict\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[1;32m    562\u001b[0m     file_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhdf5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    564\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Impute missing values in the given data with the trained model.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m        Imputed data.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     results_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(test_set, file_type\u001b[39m=\u001b[39;49mfile_type)\n\u001b[1;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m results_dict[\u001b[39m\"\u001b[39m\u001b[39mimputation\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:548\u001b[0m, in \u001b[0;36mGP_VAE.predict\u001b[0;34m(self, test_set, file_type, n_sampling_times)\u001b[0m\n\u001b[1;32m    543\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assemble_input_for_testing(data)\n\u001b[1;32m    544\u001b[0m \u001b[39m#results = self.model.forward(inputs, training=False, n_sampling_times=n_sampling_times)\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39m#imputed_data = results[\"imputed_data\"]\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[39m# embed data in latent space\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode(inputs, training\u001b[39m-\u001b[39m\u001b[39mFalse\u001b[39;00m, n_sampling_times\u001b[39m=\u001b[39mn_sampling_times)\n\u001b[1;32m    549\u001b[0m \u001b[39m# correct with gaussian process\u001b[39;00m\n\u001b[1;32m    550\u001b[0m imputed_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgp\u001b[39m.\u001b[39minfer(embedding)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GP_VAE' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "imputation = gpvae.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "gpvae.save(\"save_it_here/gpvae_physionet2012.pypots\")  # save the model for future use\n",
    "gpvae.load(\"save_it_here/gpvae_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 12:50:27 [INFO]: No given device, using default device: cpu\n",
      "2024-10-27 12:50:27 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-10-27 12:50:27 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 21,065\n",
      "2024-10-27 12:50:50 [INFO]: Epoch 001 - training loss: 11284.5031\n",
      "2024-10-27 12:51:14 [INFO]: Epoch 002 - training loss: 9194.3747\n",
      "2024-10-27 12:51:40 [INFO]: Epoch 003 - training loss: 9184.6626\n",
      "2024-10-27 12:52:04 [WARNING]: ‼️ Training got interrupted by the user. Exist now ...\n",
      "2024-10-27 12:52:04 [INFO]: Finished training. The best model is from epoch#3.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "shape of `predictions` and `targets` must match, but got (7671, 1, 48, 37) and (7671, 48, 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m imputation \u001b[39m=\u001b[39m gpvae\u001b[39m.\u001b[39mimpute(dataset)  \u001b[39m# impute the originally-missing values and artificially-missing values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m indicating_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(X) \u001b[39m^\u001b[39m np\u001b[39m.\u001b[39misnan(X_ori)  \u001b[39m# indicating mask for imputation error calculation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m mae \u001b[39m=\u001b[39m calc_mae(imputation, np\u001b[39m.\u001b[39;49mnan_to_num(X_ori), indicating_mask)  \u001b[39m# calculate mean absolute error on the ground truth (artificially-missing values)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m gpvae\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39msave_it_here/gpvae_physionet2012.pypots\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# save the model for future use\u001b[39;00m\n\u001b[1;32m     11\u001b[0m gpvae\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39msave_it_here/gpvae_physionet2012.pypots\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/utils/metrics/error.py:98\u001b[0m, in \u001b[0;36mcalc_mae\u001b[0;34m(predictions, targets, masks)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calculate the Mean Absolute Error between ``predictions`` and ``targets``.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m``masks`` can be used for filtering. For values==0 in ``masks``,\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mvalues at their corresponding positions in ``predictions`` will be ignored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m# check shapes and values of inputs\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m lib \u001b[39m=\u001b[39m _check_inputs(predictions, targets, masks)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m masks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39msum(lib\u001b[39m.\u001b[39mabs(predictions \u001b[39m-\u001b[39m targets) \u001b[39m*\u001b[39m masks) \u001b[39m/\u001b[39m (lib\u001b[39m.\u001b[39msum(masks) \u001b[39m+\u001b[39m \u001b[39m1e-12\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/utils/metrics/error.py:30\u001b[0m, in \u001b[0;36m_check_inputs\u001b[0;34m(predictions, targets, masks, check_shape)\u001b[0m\n\u001b[1;32m     28\u001b[0m target_shape \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mshape\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m check_shape:\n\u001b[0;32m---> 30\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m         prediction_shape \u001b[39m==\u001b[39m target_shape\n\u001b[1;32m     32\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape of `predictions` and `targets` must match, but got \u001b[39m\u001b[39m{\u001b[39;00mprediction_shape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtarget_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[39m# check NaN\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39misnan(predictions)\u001b[39m.\u001b[39many(), \u001b[39m\"\u001b[39m\u001b[39m`predictions` mustn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain NaN values, but detected NaN in it\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: shape of `predictions` and `targets` must match, but got (7671, 1, 48, 37) and (7671, 48, 37)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "from pypots.imputation import GPVAE\n",
    "from pypots.utils.metrics import calc_mae\n",
    "gpvae = GPVAE(n_steps=48, n_features=37, latent_size = 12, epochs=10)\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "gpvae.fit(dataset)  # train the model on the dataset\n",
    "imputation = gpvae.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "gpvae.save(\"save_it_here/gpvae_physionet2012.pypots\")  # save the model for future use\n",
    "gpvae.load(\"save_it_here/gpvae_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 14:06:48 [INFO]: No given device, using default device: cpu\n",
      "2024-10-09 14:06:48 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-10-09 14:06:48 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,378,358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7671, 48, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 14:08:43 [INFO]: Epoch 001 - training loss: 1.2981\n",
      "2024-10-09 14:10:22 [INFO]: Epoch 002 - training loss: 0.3698\n",
      "2024-10-09 14:11:53 [INFO]: Epoch 003 - training loss: 0.3274\n",
      "2024-10-09 14:13:24 [INFO]: Epoch 004 - training loss: 0.3104\n",
      "2024-10-09 14:14:56 [INFO]: Epoch 005 - training loss: 0.3024\n",
      "2024-10-09 14:16:25 [INFO]: Epoch 006 - training loss: 0.2977\n",
      "2024-10-09 14:18:08 [INFO]: Epoch 007 - training loss: 0.2921\n",
      "2024-10-09 14:19:42 [INFO]: Epoch 008 - training loss: 0.2838\n",
      "2024-10-09 14:21:32 [INFO]: Epoch 009 - training loss: 0.2714\n",
      "2024-10-09 14:23:10 [INFO]: Epoch 010 - training loss: 0.2671\n",
      "2024-10-09 14:23:10 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2024-10-09 14:23:43 [INFO]: Successfully created the given path save_it_here\n",
      "2024-10-09 14:23:43 [INFO]: Saved the model to save_it_here/saits_physionet2012.pypots\n",
      "2024-10-09 14:23:43 [INFO]: Model loaded successfully from save_it_here/saits_physionet2012.pypots\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "from pypots.imputation import SAITS\n",
    "from pypots.utils.metrics import calc_mae\n",
    "gpvae = SAITS(n_steps=48, n_features=37, n_layers=2, d_model=256, n_heads=4, d_k=64, d_v=64, d_ffn=128, dropout=0.1, epochs=10)\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "gpvae.fit(dataset)  # train the model on the dataset\n",
    "imputation = gpvae.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "gpvae.save(\"save_it_here/saits_physionet2012.pypots\")  # save the model for future use\n",
    "gpvae.load(\"save_it_here/saits_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPOTS Environment",
   "language": "python",
   "name": "pypots-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
