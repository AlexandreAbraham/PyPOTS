{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running example from the readme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:12:39 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Data_Beans)...\n",
      "2024-11-07 16:12:39 [INFO]: Starting preprocessing physionet_2012...\n",
      "2024-11-07 16:12:39 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2024-11-07 16:12:39 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2024-11-07 16:12:39 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2024-11-07 16:12:39 [INFO]: Loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:12:49 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2024-11-07 16:12:49 [INFO]: 69287 values masked out in the val set as ground truth, take 10.03% of the original observed values\n",
      "2024-11-07 16:12:49 [INFO]: 86029 values masked out in the test set as ground truth, take 10.03% of the original observed values\n",
      "2024-11-07 16:12:49 [INFO]: Total sample number: 11988\n",
      "2024-11-07 16:12:49 [INFO]: Training set size: 7671 (63.99%)\n",
      "2024-11-07 16:12:49 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2024-11-07 16:12:49 [INFO]: Test set size: 2399 (20.01%)\n",
      "2024-11-07 16:12:49 [INFO]: Number of steps: 48\n",
      "2024-11-07 16:12:49 [INFO]: Number of features: 37\n",
      "2024-11-07 16:12:49 [INFO]: Train set missing rate: 79.66%\n",
      "2024-11-07 16:12:49 [INFO]: Validating set missing rate: 81.76%\n",
      "2024-11-07 16:12:49 [INFO]: Test set missing rate: 81.88%\n",
      "2024-11-07 16:12:49 [WARNING]: ⚠️ load_specific_dataset() will be deprecated in the near future. Data preprocessing functions are moved to BenchPOTS, which now supports processing 170+ public time-series datasets.\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing. Tedious, but PyPOTS can help.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pygrinder import mcar\n",
    "from pypots.data import load_specific_dataset\n",
    "data = load_specific_dataset('physionet_2012')  # PyPOTS will automatically download and extract it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7671, 48, 37)\n"
     ]
    }
   ],
   "source": [
    "X = data['train_X']\n",
    "scaler = data['scaler']\n",
    "X_shape = X.shape\n",
    "X = scaler.transform(X.reshape(-1,X_shape[2])).reshape(X_shape)\n",
    "\n",
    "## normalize\n",
    "normalize = True\n",
    "if normalize and False:\n",
    "    latent_dim = X.shape[2]\n",
    "    mean, std = np.zeros((1,1,latent_dim)), np.ones((1,1,latent_dim))\n",
    "    for l in range(latent_dim):\n",
    "        X_l = X[:,:,l][X[:,:,l]==X[:,:,l]] \n",
    "        if len(X_l) > 0:   \n",
    "            mean[:,:,l] = X_l.mean()    \n",
    "            std[:,:,l] = X_l.std()   \n",
    "    X = (X - mean) / std\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "rng = np.random.RandomState(0)\n",
    "qt = QuantileTransformer(n_quantiles=10, random_state=0, output_distribution = 'normal')\n",
    "\n",
    "if normalize:\n",
    "    n_batch, n_obs, latent_dim = X.shape\n",
    "    for l in range(latent_dim):\n",
    "        X_l = X[:,:,l][X[:,:,l]==X[:,:,l]] \n",
    "        qt.fit(X_l.reshape(-1,1))\n",
    "        \n",
    "        X[:,:,l] = qt.transform(X[:,:,l].reshape(-1,1)).reshape(n_batch, n_obs)\n",
    "\n",
    "\n",
    "X_ori = X  # keep X_ori for validation\n",
    "X = mcar(X, 0.1)  # randomly hold out 10% observed values as ground truth\n",
    "dataset = {\"X\": X[:2000]}  # X for model input\n",
    "print(X.shape)  # (11988, 48, 37), 11988 samples and each sample has 48 time steps, 37 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:13:29 [INFO]: No given device, using default device: cpu\n",
      "2024-11-07 16:13:29 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-11-07 16:13:29 [INFO]: GP_VAE initialized with the given hyperparameters, the number of trainable parameters: 16,485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dimensions is: \n",
      "GpvaeEncoder(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (mu_layer): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (logvar_layer): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "GpvaeDecoder(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=37, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:13:42 [INFO]: Epoch 001 - training loss: 369.9288\n",
      "2024-11-07 16:13:52 [INFO]: Epoch 002 - training loss: 262.6248\n",
      "2024-11-07 16:14:02 [INFO]: Epoch 003 - training loss: 243.6871\n",
      "2024-11-07 16:14:14 [INFO]: Epoch 004 - training loss: 233.5115\n",
      "2024-11-07 16:14:23 [INFO]: Epoch 005 - training loss: 227.1966\n",
      "2024-11-07 16:14:23 [INFO]: Finished training. The best model is from epoch#5.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "from pypots.imputation import GP_VAE\n",
    "from pypots.utils.metrics import calc_mae\n",
    "from pypots.optim.adam import Adam\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "\n",
    "gpvae = GP_VAE(n_steps = 48, \n",
    "            n_features = 37, \n",
    "            latent_size = 16, \n",
    "            epochs = 5, \n",
    "            batch_size = 8,\n",
    "            beta = .01, \n",
    "            K = 10,  \n",
    "            encoder_sizes = (64, 64), \n",
    "            decoder_sizes = (64, 64),\n",
    "            optimizer = Adam(weight_decay = 1e-4) #, lr_scheduler = LRScheduler\n",
    "            )\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "gpvae.fit(dataset)  # train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting kernel\n",
      "torch.Size([8, 48])\n",
      "torch.Size([8, 8, 8]) torch.Size([8, 48])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (48) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgpytorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m gpytorch\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mcholesky_jitter(\u001b[39m1e-4\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     gpvae\u001b[39m.\u001b[39;49mfit_kernel(dataset)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:514\u001b[0m, in \u001b[0;36mGP_VAE.fit_kernel\u001b[0;34m(self, train_set, val_set, file_type)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# Step 2bis: learn the kernel\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfitting kernel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 514\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_kernel(training_loader)\n\u001b[1;32m    516\u001b[0m \u001b[39m# Step 3: save the model if necessary\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_auto_save_model_if_necessary(confirm_saving\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_saving_strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:484\u001b[0m, in \u001b[0;36mGP_VAE._fit_kernel\u001b[0;34m(self, training_loader)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_kernel\u001b[39m(\n\u001b[1;32m    481\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    482\u001b[0m     training_loader) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgp\u001b[39m.\u001b[39;49mfit_kernel(training_loader)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/gp_model.py:171\u001b[0m, in \u001b[0;36mProbabilisticGP.fit_kernel\u001b[0;34m(self, training_loader, training_iter)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m#help(out)\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mcovariance_matrix\u001b[39m.\u001b[39mshape, z_mu[:,:,j]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 171\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmll[j](out, z_mu[:,:,j])\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_params[j]) \u001b[39m>\u001b[39m \u001b[39m30\u001b[39m \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mabs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_params[j][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_params[j])[\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mmean()) \u001b[39m<\u001b[39m \u001b[39m1e-3\u001b[39m:\n\u001b[1;32m    174\u001b[0m     dims_to_train\u001b[39m.\u001b[39mremove(j)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:82\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNaN observation policy \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfill\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[39m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m res \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mlog_prob(target)\n\u001b[1;32m     83\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     85\u001b[0m \u001b[39m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_sample(value)\n\u001b[1;32m    176\u001b[0m mean, covar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlazy_covariance_matrix\n\u001b[0;32m--> 177\u001b[0m diff \u001b[39m=\u001b[39m value \u001b[39m-\u001b[39;49m mean\n\u001b[1;32m    179\u001b[0m \u001b[39m# Repeat the covar to match the batch shape of diff\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m diff\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m covar\u001b[39m.\u001b[39mbatch_shape:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (48) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "with gpytorch.settings.cholesky_jitter(1e-4):\n",
    "    gpvae.fit_kernel(dataset)  # train the model on the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pypots.imputation.gp_ae.model.ProbabilisticGP at 0x2a16fb100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpvae.gp._fit_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GP_VAE' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m imputation \u001b[39m=\u001b[39m gpvae\u001b[39m.\u001b[39;49mimpute(dataset)  \u001b[39m# impute the originally-missing values and artificially-missing values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m indicating_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(X) \u001b[39m^\u001b[39m np\u001b[39m.\u001b[39misnan(X_ori)  \u001b[39m# indicating mask for imputation error calculation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mae \u001b[39m=\u001b[39m calc_mae(imputation, np\u001b[39m.\u001b[39mnan_to_num(X_ori), indicating_mask)  \u001b[39m# calculate mean absolute error on the ground truth (artificially-missing values)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:581\u001b[0m, in \u001b[0;36mGP_VAE.impute\u001b[0;34m(self, test_set, file_type)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimpute\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    561\u001b[0m     test_set: Union[\u001b[39mdict\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[1;32m    562\u001b[0m     file_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhdf5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    564\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Impute missing values in the given data with the trained model.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m        Imputed data.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     results_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(test_set, file_type\u001b[39m=\u001b[39;49mfile_type)\n\u001b[1;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m results_dict[\u001b[39m\"\u001b[39m\u001b[39mimputation\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/imputation/gp_ae/model.py:548\u001b[0m, in \u001b[0;36mGP_VAE.predict\u001b[0;34m(self, test_set, file_type, n_sampling_times)\u001b[0m\n\u001b[1;32m    543\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assemble_input_for_testing(data)\n\u001b[1;32m    544\u001b[0m \u001b[39m#results = self.model.forward(inputs, training=False, n_sampling_times=n_sampling_times)\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39m#imputed_data = results[\"imputed_data\"]\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[39m# embed data in latent space\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode(inputs, training\u001b[39m-\u001b[39m\u001b[39mFalse\u001b[39;00m, n_sampling_times\u001b[39m=\u001b[39mn_sampling_times)\n\u001b[1;32m    549\u001b[0m \u001b[39m# correct with gaussian process\u001b[39;00m\n\u001b[1;32m    550\u001b[0m imputed_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgp\u001b[39m.\u001b[39minfer(embedding)\n",
      "File \u001b[0;32m~/miniforge3/envs/pypots-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GP_VAE' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "imputation = gpvae.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "gpvae.save(\"save_it_here/gpvae_physionet2012.pypots\")  # save the model for future use\n",
    "gpvae.load(\"save_it_here/gpvae_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 12:50:27 [INFO]: No given device, using default device: cpu\n",
      "2024-10-27 12:50:27 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-10-27 12:50:27 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 21,065\n",
      "2024-10-27 12:50:50 [INFO]: Epoch 001 - training loss: 11284.5031\n",
      "2024-10-27 12:51:14 [INFO]: Epoch 002 - training loss: 9194.3747\n",
      "2024-10-27 12:51:40 [INFO]: Epoch 003 - training loss: 9184.6626\n",
      "2024-10-27 12:52:04 [WARNING]: ‼️ Training got interrupted by the user. Exist now ...\n",
      "2024-10-27 12:52:04 [INFO]: Finished training. The best model is from epoch#3.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "shape of `predictions` and `targets` must match, but got (7671, 1, 48, 37) and (7671, 48, 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m imputation \u001b[39m=\u001b[39m gpvae\u001b[39m.\u001b[39mimpute(dataset)  \u001b[39m# impute the originally-missing values and artificially-missing values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m indicating_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(X) \u001b[39m^\u001b[39m np\u001b[39m.\u001b[39misnan(X_ori)  \u001b[39m# indicating mask for imputation error calculation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m mae \u001b[39m=\u001b[39m calc_mae(imputation, np\u001b[39m.\u001b[39;49mnan_to_num(X_ori), indicating_mask)  \u001b[39m# calculate mean absolute error on the ground truth (artificially-missing values)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m gpvae\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39msave_it_here/gpvae_physionet2012.pypots\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# save the model for future use\u001b[39;00m\n\u001b[1;32m     11\u001b[0m gpvae\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39msave_it_here/gpvae_physionet2012.pypots\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/utils/metrics/error.py:98\u001b[0m, in \u001b[0;36mcalc_mae\u001b[0;34m(predictions, targets, masks)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calculate the Mean Absolute Error between ``predictions`` and ``targets``.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m``masks`` can be used for filtering. For values==0 in ``masks``,\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mvalues at their corresponding positions in ``predictions`` will be ignored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m# check shapes and values of inputs\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m lib \u001b[39m=\u001b[39m _check_inputs(predictions, targets, masks)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m masks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39msum(lib\u001b[39m.\u001b[39mabs(predictions \u001b[39m-\u001b[39m targets) \u001b[39m*\u001b[39m masks) \u001b[39m/\u001b[39m (lib\u001b[39m.\u001b[39msum(masks) \u001b[39m+\u001b[39m \u001b[39m1e-12\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phd/Code_pour_manuscript/PyPOTS/pypots/utils/metrics/error.py:30\u001b[0m, in \u001b[0;36m_check_inputs\u001b[0;34m(predictions, targets, masks, check_shape)\u001b[0m\n\u001b[1;32m     28\u001b[0m target_shape \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mshape\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m check_shape:\n\u001b[0;32m---> 30\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m         prediction_shape \u001b[39m==\u001b[39m target_shape\n\u001b[1;32m     32\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape of `predictions` and `targets` must match, but got \u001b[39m\u001b[39m{\u001b[39;00mprediction_shape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtarget_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[39m# check NaN\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39misnan(predictions)\u001b[39m.\u001b[39many(), \u001b[39m\"\u001b[39m\u001b[39m`predictions` mustn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain NaN values, but detected NaN in it\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: shape of `predictions` and `targets` must match, but got (7671, 1, 48, 37) and (7671, 48, 37)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "from pypots.imputation import GPVAE\n",
    "from pypots.utils.metrics import calc_mae\n",
    "gpvae = GPVAE(n_steps=48, n_features=37, latent_size = 12, epochs=10)\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "gpvae.fit(dataset)  # train the model on the dataset\n",
    "imputation = gpvae.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "gpvae.save(\"save_it_here/gpvae_physionet2012.pypots\")  # save the model for future use\n",
    "gpvae.load(\"save_it_here/gpvae_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 14:06:48 [INFO]: No given device, using default device: cpu\n",
      "2024-10-09 14:06:48 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-10-09 14:06:48 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,378,358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7671, 48, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 14:08:43 [INFO]: Epoch 001 - training loss: 1.2981\n",
      "2024-10-09 14:10:22 [INFO]: Epoch 002 - training loss: 0.3698\n",
      "2024-10-09 14:11:53 [INFO]: Epoch 003 - training loss: 0.3274\n",
      "2024-10-09 14:13:24 [INFO]: Epoch 004 - training loss: 0.3104\n",
      "2024-10-09 14:14:56 [INFO]: Epoch 005 - training loss: 0.3024\n",
      "2024-10-09 14:16:25 [INFO]: Epoch 006 - training loss: 0.2977\n",
      "2024-10-09 14:18:08 [INFO]: Epoch 007 - training loss: 0.2921\n",
      "2024-10-09 14:19:42 [INFO]: Epoch 008 - training loss: 0.2838\n",
      "2024-10-09 14:21:32 [INFO]: Epoch 009 - training loss: 0.2714\n",
      "2024-10-09 14:23:10 [INFO]: Epoch 010 - training loss: 0.2671\n",
      "2024-10-09 14:23:10 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2024-10-09 14:23:43 [INFO]: Successfully created the given path save_it_here\n",
      "2024-10-09 14:23:43 [INFO]: Saved the model to save_it_here/saits_physionet2012.pypots\n",
      "2024-10-09 14:23:43 [INFO]: Model loaded successfully from save_it_here/saits_physionet2012.pypots\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "from pypots.imputation import SAITS\n",
    "from pypots.utils.metrics import calc_mae\n",
    "gpvae = SAITS(n_steps=48, n_features=37, n_layers=2, d_model=256, n_heads=4, d_k=64, d_v=64, d_ffn=128, dropout=0.1, epochs=10)\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "gpvae.fit(dataset)  # train the model on the dataset\n",
    "imputation = gpvae.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "gpvae.save(\"save_it_here/saits_physionet2012.pypots\")  # save the model for future use\n",
    "gpvae.load(\"save_it_here/saits_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPOTS Environment",
   "language": "python",
   "name": "pypots-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
